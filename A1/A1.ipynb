{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_1 = pd.read_csv(\"data/info_1.csv\", header=None) \n",
    "info_2 = pd.read_csv(\"data/info_2.csv\", header=None) \n",
    "test_no_label_1 = pd.read_csv(\"data/test_no_label_1.csv\", header=None) \n",
    "test_no_label_2 = pd.read_csv(\"data/test_no_label_2.csv\",header=None) \n",
    "test_with_label_1 = pd.read_csv(\"data/test_with_label_1.csv\", header=None) \n",
    "test_with_label_2 = pd.read_csv(\"data/test_with_label_2.csv\", header=None) \n",
    "train_1 = pd.read_csv(\"data/train_1.csv\", header=None) \n",
    "train_2 = pd.read_csv(\"data/train_2.csv\", header=None) \n",
    "val_1 = pd.read_csv(\"data/val_1.csv\", header=None) \n",
    "val_2 = pd.read_csv(\"data/val_2.csv\", header=None) \n",
    "sample_submission = pd.read_csv(\"data/sample_submission.csv\", header=None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition the data sets 1 & 2 into train, val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set 1\n",
    "X_train_1 = train_1.iloc[:, 0:1024].to_numpy()\n",
    "Y_train_1 = train_1.iloc[:, 1024:1025].to_numpy().ravel()\n",
    "\n",
    "X_val_1 = val_1.iloc[:, 0:1024].to_numpy()\n",
    "Y_val_1 = val_1.iloc[:, 1024:1025].to_numpy().ravel()\n",
    "\n",
    "X_test_1 = test_with_label_1.iloc[:, 0:1024].to_numpy()\n",
    "Y_test_1 = test_with_label_1.iloc[:, 1024:1025].to_numpy().ravel()\n",
    "\n",
    "# Data set 2\n",
    "X_train_2 = train_2.iloc[:, 0:1024].to_numpy()\n",
    "Y_train_2 = train_2.iloc[:, 1024:1025].to_numpy().ravel()\n",
    "\n",
    "X_val_2 = val_2.iloc[:, 0:1024].to_numpy()\n",
    "Y_val_2 = val_2.iloc[:, 1024:1025].to_numpy().ravel()\n",
    "\n",
    "X_test_2 = test_with_label_2.iloc[:, 0:1024].to_numpy()\n",
    "Y_test_2 = test_with_label_2.iloc[:, 1024:1025].to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Imports for GNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# DS1\n",
    "trained_model_1 = gnb.fit(X_train_1, Y_train_1)\n",
    "Y_pred_1 = trained_model_1.predict(X_test_1)\n",
    "\n",
    "# DS2\n",
    "trained_model_2 = gnb.fit(X_train_2, Y_train_2)\n",
    "Y_pred_2 = trained_model_2.predict(X_test_2)\n",
    "\n",
    "# Outputs\n",
    "GNB_DS1 = pd.DataFrame(data=Y_pred_1)\n",
    "GNB_DS1.index = GNB_DS1.index + 1\n",
    "GNB_DS1.to_csv(\"output_files/GNB-DS1.csv\", header=None)\n",
    "\n",
    "GNB_DS2 = pd.DataFrame(data=Y_pred_2)\n",
    "GNB_DS2.index = GNB_DS2.index + 1\n",
    "GNB_DS2.to_csv(\"output_files/GNB-DS2.csv\", header=None)\n",
    "\n",
    "f_GNB_DS1 = open(\"output_files/GNB-DS1.csv\", \"a\")\n",
    "conf_1 = confusion_matrix(Y_test_1, Y_pred_1)\n",
    "report_1 = classification_report(Y_test_1, Y_pred_1)\n",
    "f_GNB_DS1.write(\"\\nConfusion Matrix:\\n\\n\")\n",
    "f_GNB_DS1.write(str(conf_1))\n",
    "f_GNB_DS1.write(\"\\n\\nClassification Report:\\n\\n\")\n",
    "f_GNB_DS1.write(str(report_1))\n",
    "f_GNB_DS1.close()\n",
    "\n",
    "f_GNB_DS2 = open(\"output_files/GNB-DS2.csv\", \"a\")\n",
    "conf_2 = confusion_matrix(Y_test_2, Y_pred_2)\n",
    "report_2 = classification_report(Y_test_2, Y_pred_2)\n",
    "f_GNB_DS2.write(\"\\nConfusion Matrix:\\n\\n\")\n",
    "f_GNB_DS2.write(str(conf_2))\n",
    "f_GNB_DS2.write(\"\\n\\nClassification Report:\\n\\n\")\n",
    "f_GNB_DS2.write(str(report_2))\n",
    "f_GNB_DS2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "- The warning is generated (from the first dataset) because the letter E corresponding to value 4 \n",
    "is never predicted by our model when it should have been predicted 2 times\n",
    "in the Y_test_1.\n",
    "- This leads to a denominator of 0 in the precision calculation and the same for the F1 measure therefore the report defaults these values to 0.0 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base-DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Imports for GNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "base_dt = DecisionTreeClassifier(criterion=\"entropy\", random_state=0)\n",
    "\n",
    "# DS1\n",
    "trained_model_1 = base_dt.fit(X_train_1, Y_train_1)\n",
    "Y_pred_1 = trained_model_1.predict(X_test_1)\n",
    "\n",
    "# DS2\n",
    "trained_model_2 = base_dt.fit(X_train_2, Y_train_2)\n",
    "Y_pred_2 = trained_model_2.predict(X_test_2)\n",
    "\n",
    "# Outputs\n",
    "Base_DT_DS1 = pd.DataFrame(data=Y_pred_1)\n",
    "Base_DT_DS1.index = Base_DT_DS1.index + 1\n",
    "Base_DT_DS1.to_csv(\"output_files/Base-DT-DS1.csv\", header=None)\n",
    "\n",
    "Base_DT_DS2 = pd.DataFrame(data=Y_pred_2)\n",
    "Base_DT_DS2.index = Base_DT_DS2.index + 1\n",
    "Base_DT_DS2.to_csv(\"output_files/Base-DT-DS2.csv\", header=None)\n",
    "\n",
    "f_Base_DT_DS1 = open(\"output_files/Base-DT-DS1.csv\", \"a\")\n",
    "conf_1 = confusion_matrix(Y_test_1, Y_pred_1)\n",
    "report_1 = classification_report(Y_test_1, Y_pred_1)\n",
    "f_Base_DT_DS1.write(\"\\nConfusion Matrix:\\n\\n\")\n",
    "f_Base_DT_DS1.write(str(conf_1))\n",
    "f_Base_DT_DS1.write(\"\\n\\nClassification Report:\\n\\n\")\n",
    "f_Base_DT_DS1.write(str(report_1))\n",
    "f_Base_DT_DS1.close()\n",
    "\n",
    "f_Base_DT_DS2 = open(\"output_files/Base-DT-DS2.csv\", \"a\")\n",
    "conf_2 = confusion_matrix(Y_test_2, Y_pred_2)\n",
    "report_2 = classification_report(Y_test_2, Y_pred_2)\n",
    "f_Base_DT_DS2.write(\"\\nConfusion Matrix:\\n\\n\")\n",
    "f_Base_DT_DS2.write(str(conf_2))\n",
    "f_Base_DT_DS2.write(\"\\n\\nClassification Report:\\n\\n\")\n",
    "f_Base_DT_DS2.write(str(report_2))\n",
    "f_Base_DT_DS2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best-DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for GNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#todo remove\n",
    "from sklearn import svm\n",
    "svc = svm.SVC()\n",
    "\n",
    "best_dt = DecisionTreeClassifier(criterion=\"entropy\", random_state=0)\n",
    "\n",
    "# DS1\n",
    "trained_model_1 = best_dt.fit(X_train_1, Y_train_1)\n",
    "Y_pred_1 = trained_model_1.predict(X_test_1)\n",
    "\n",
    "# DS2\n",
    "trained_model_2 = best_dt.fit(X_train_2, Y_train_2)\n",
    "Y_pred_2 = trained_model_2.predict(X_test_2)\n",
    "\n",
    "# Outputs\n",
    "Best_DT_DS1 = pd.DataFrame(data=Y_pred_1)\n",
    "Best_DT_DS1.index = Best_DT_DS1.index + 1\n",
    "Best_DT_DS1.to_csv(\"output_files/Best-DT-DS1.csv\", header=None)\n",
    "\n",
    "Best_DT_DS2 = pd.DataFrame(data=Y_pred_2)\n",
    "Best_DT_DS2.index = Best_DT_DS2.index + 1\n",
    "Best_DT_DS2.to_csv(\"output_files/Best-DT-DS2.csv\", header=None)\n",
    "\n",
    "f_Best_DT_DS1 = open(\"output_files/Best-DT-DS1.csv\", \"a\")\n",
    "conf_1 = confusion_matrix(Y_test_1, Y_pred_1)\n",
    "report_1 = classification_report(Y_test_1, Y_pred_1)\n",
    "f_Best_DT_DS1.write(\"\\nConfusion Matrix:\\n\\n\")\n",
    "f_Best_DT_DS1.write(str(conf_1))\n",
    "f_Best_DT_DS1.write(\"\\n\\nClassification Report:\\n\\n\")\n",
    "f_Best_DT_DS1.write(str(report_1))\n",
    "f_Best_DT_DS1.close()\n",
    "\n",
    "f_Best_DT_DS2 = open(\"output_files/Best-DT-DS2.csv\", \"a\")\n",
    "conf_2 = confusion_matrix(Y_test_2, Y_pred_2)\n",
    "report_2 = classification_report(Y_test_2, Y_pred_2)\n",
    "f_Best_DT_DS2.write(\"\\nConfusion Matrix:\\n\\n\")\n",
    "f_Best_DT_DS2.write(str(conf_2))\n",
    "f_Best_DT_DS2.write(\"\\n\\nClassification Report:\\n\\n\")\n",
    "f_Best_DT_DS2.write(str(report_2))\n",
    "f_Best_DT_DS2.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
